1. initial was direction and distance to checkpoint and position of hceckpoint and punishment for wall and time and reward for checkpoint.
2. Added direction vector from position of car to the position of the checkpoint and lowered punishment for wall collision from 0.5 to 0.3 and slightly increased reward from 0.25 to 0.3. switched neural network to only have 1 layer instead of 2 and to have 64 hidden nodes instead of 148 which made a significant imporvement in speed of learning.
3. Added a dot product of the up vector (front of the car) and the checkpoint indicating if the car is facing the checkpoint and removed punishment for time.
4. Increased the punishment to -1 and added a 1 reward for completing all checkpoints. Added back the punishment for time to ensure cars dont take too long.
5. Removed the direction observation and agents starting demonstrating very interesting behaviour. They crash into walls significantly less and learn faster. They've also started taking shortcuts to increase the speed of each lap. It took only 300,000 steps for them to get to the part the other training sets got stuck at. step 450,000 two cars have managed to clear the maze and make it near the end. Step 1,000,000 one car almost cleared the whole track except for last checkpoint. Some agents have started instantly crashing, I'm guessing this is due to time punihsing too much and checkpoints not rewarding enough so the cars have realized they get more if they just instantly crash and dont get punished for taking too long.
6. Added Speed and rotation to observation so the agents can have the information to adjust to certain turns. Reduced the time punishment further and improved the reward for capturing checkpoints. The initial performance was good and the issue of cars crashing has been fixed, but the cars were not able to overcome maze even after 2 million steps.
7. Removed rotation and added normalised velocity instead of normal velocity and normalised distance as neural networks work better with values between 0 and 1. Also added checkpoints to ray cast so the cars can detect the checkpoint. The results were significantly worse. Adding the checkpoint to ray cast was making the agents hesitate to get close to checkpoints.

8. Added dot product of the checkpoint right and car up which is 1 if the car is directly looking at the checkpoint and -1 if the car is facing the opposite way. Also added 2 more checkpoints in the turn as cars struggled to learn in the beggining.

9. had to increase curiosity, change stacked vector, change what it can see and gets rewarded, tried 2 or 1 layers, and increased buffer size since action is continious. Problem was crashing as it did not let htem explore more options